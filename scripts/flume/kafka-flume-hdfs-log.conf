## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = bigdata01:9092,bigdata02:9092,bigdata03:9092
a1.sources.r1.kafka.topics=log-gmall
# 配置拦截器，可以用于解决零点漂移
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.itcode.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/software/flume-1.10.1/checkpoint/behavior1
a1.channels.c1.dataDirs = /opt/software/flume-1.10.1/data/behavior1/


## sink1 : hdfs要求flume event的header中存在timestamp, 如果我们不重置，那么会使用本地时间。
a1.sinks.k1.type = hdfs
# 默认会采用执行flume脚本的unix机器的时间，可能会存在零点漂移的问题，我们可以使用flume拦截器添加时间戳到flume event的header中
a1.sinks.k1.hdfs.path = /origin_data/apps/gmall/offline/log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

#控制生成的小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是压缩之后的文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = gzip

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

